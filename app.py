import os
import math
from pathlib import Path

import pandas as pd
import streamlit as st

from core.tokenizer import estimate_tokens
from core.pricing import DEFAULT_PRICING, estimate_cost
from core.glossary import load_glossary_csv, build_patterns, find_hits
from core.rules import parse_rules_yaml
from core.prompts import build_system_prompt
from core.qc import quick_qc
from core.translator import Translator
from core.batching import chapters_from_folder, build_batch_jsonl

st.set_page_config(page_title="Star Trek ç¿»è¯‘åŠ©æ‰‹ Â· GUI & Batch", layout="wide")

# ===== Sidebar: åŸºæœ¬è®¾ç½® =====
st.sidebar.header("è®¾ç½®")
model = st.sidebar.selectbox("é€‰æ‹©æ¨¡å‹", ["gpt-5-mini", "gpt-5"], index=0)
batch_mode = st.sidebar.checkbox("Batch APIï¼ˆçº¦ -20% æˆæœ¬ï¼‰", value=True)
batch_discount = 0.20 if batch_mode else 0.0

# ä»…ä¼šè¯å†…å­˜å‚¨ï¼Œä¸è½ç›˜
api_key = st.sidebar.text_input("OpenAI API Key", type="password", help="ä»…å½“å‰ä¼šè¯ä½¿ç”¨ï¼›ç•™ç©ºåˆ™å°è¯•ä½¿ç”¨ç¯å¢ƒå˜é‡ OPENAI_API_KEY")



# è®¡ä»·è®¾ç½®
pricing = {}
for m in ["gpt-5-mini", "gpt-5"]:
    col1, col2 = st.sidebar.columns(2)
    with col1:
        pi = st.number_input(f"{m} è¾“å…¥($/1M)", value=float(DEFAULT_PRICING[m]["input"]), min_value=0.0, step=0.05, key=f"pi_{m}")
    with col2:
        po = st.number_input(f"{m} è¾“å‡º($/1M)", value=float(DEFAULT_PRICING[m]["output"]), min_value=0.0, step=0.10, key=f"po_{m}")
    pricing[m] = {"input": pi, "output": po}

out_multiplier = st.sidebar.slider("è¾“å‡ºtoken/è¾“å…¥æ¯”ä¾‹", 1.05, 1.30, 1.10, 0.01)

st.title("ğŸ–– æ˜Ÿé™…è¿·èˆªç¿»è¯‘åŠ©æ‰‹ï¼šå•ç«  + æ‰¹é‡")

# ===== Tabs =====
T1, T2, T3, T4 = st.tabs(["â‘  ç²˜è´´æ•´ç« ä¼°ä»·", "â‘¡ æœ¯è¯­ä¸äººå", "â‘¢ è§„åˆ™ä¸ç³»ç»Ÿæç¤º", "â‘£ æ‰¹å¤„ç†/JSONL ç”Ÿæˆ"])

# --- Tab1: ä¼°ä»· ---
with T1:
    st.subheader("æ•´ç« è‹±æ–‡å†…å®¹")
    chapter_text = st.text_area("ç²˜è´´è‹±æ–‡åŸæ–‡ï¼š", height=300, placeholder="Paste chapter text hereâ€¦")
    if st.button("è®¡ç®—æˆæœ¬", type="primary"):
        if not chapter_text.strip():
            st.warning("è¯·å…ˆç²˜è´´æ–‡æœ¬ã€‚")
        else:
            in_tokens = estimate_tokens(chapter_text, model)
            out_tokens = math.ceil(in_tokens * out_multiplier)
            cb = estimate_cost(model, in_tokens, out_tokens, pricing, batch_discount)
            c1, c2, c3 = st.columns(3)
            c1.metric("è¾“å…¥ tokens", f"{cb.input_tokens:,}")
            c2.metric("è¾“å‡º tokens(ä¼°)", f"{cb.output_tokens:,}")
            c3.metric("é¢„è®¡æˆæœ¬(USD)", f"{cb.total_cost:.2f}")
            st.caption(f"æ¨¡å‹ï¼š{model} Â· Batchï¼š{'ON' if batch_mode else 'OFF'} Â· è¾“å‡ºå€ç‡ï¼š{out_multiplier:.2f}")

# --- Tab2: æœ¯è¯­ & äººåï¼ˆå«ç¼–è¾‘ï¼‰ ---
with T2:
    st.subheader("æœ¯è¯­è¡¨ (CSVï¼Œåˆ—: source,target,type,note)")
    up = st.file_uploader("ä¸Šä¼ æœ¯è¯­è¡¨ CSVï¼ˆå¯é€‰ï¼‰", type=["csv"], key="csv_up")
    sample_path = Path("data/glossary_sample.csv")
    fallback_df = pd.read_csv(sample_path) if sample_path.exists() else pd.DataFrame([
        {"source":"U.S.S. Enterprise","target":"è”é‚¦æ˜Ÿèˆ°ä¼ä¸šå·","type":"ship","note":"èˆ°åç¿»è¯‘"},
        {"source":"Enterprise","target":"ä¼ä¸šå·","type":"ship","note":"èˆ°åç¿»è¯‘"},
        {"source":"U.S.S. Titan","target":"è”é‚¦æ˜Ÿèˆ°æ³°å¦å·","type":"ship","note":"èˆ°åç¿»è¯‘"},
        {"source":"Titan","target":"æ³°å¦å·","type":"ship","note":"èˆ°åç¿»è¯‘"},
        {"source":"U.S.S. Aventine","target":"è”é‚¦æ˜Ÿèˆ°å®‰æ–‡å©·å·","type":"ship","note":"èˆ°åç¿»è¯‘"},
        {"source":"Aventine","target":"å®‰æ–‡å©·å·","type":"ship","note":"èˆ°åç¿»è¯‘"},
        {"source":"Borg","target":"åšæ ¼","type":"species","note":"ç‰©ç§"},
        {"source":"Borg drone","target":"åšæ ¼ä¸ªä½“","type":"ship","note":""},
        {"source":"Starfleet","target":"æ˜Ÿé™…èˆ°é˜Ÿ","type":"org","note":""},
        {"source":"Captain","target":"ä¸Šæ ¡","type":"rank","note":""},
        {"source":"Commander","target":"ä¸­æ ¡","type":"rank","note":""},
        {"source":"Lieutenant Commander","target":"å°‘æ ¡","type":"rank","note":""},
        {"source":"Operations manager","target":"æ“ä½œå®˜","type":"role","note":""},
        {"source":"Security chief","target":"å®‰å…¨å®˜","type":"role","note":""},
        {"source":"Flight controller","target":"èˆµæ‰‹","type":"role","note":""},
        {"source":"Number One","target":"å¤§å‰¯","type":"role","note":""},
        {"source":"Chief engineer","target":"è½®æœºé•¿","type":"role","note":""},
        {"source":"turbolift ","target":"æ¶¡è½®ç”µæ¢¯","type":"item","note":""},
    ])
    glossary_df = load_glossary_csv(up, fallback_df)
    # è‹¥ç”¨æˆ·å·²ç¼–è¾‘ï¼Œä¼˜å…ˆä½¿ç”¨ä¼šè¯ç‰ˆæœ¬ï¼Œå¹¶å»æ‰ä¸å¯åºåˆ—åŒ–åˆ—
    if "glossary_df" in st.session_state:
        glossary_df = st.session_state["glossary_df"].copy()
    # ğŸ§¹ é˜²å¾¡æ€§æ¸…ç†ï¼šå»æ‰ç¼–è¯‘åçš„æ­£åˆ™åˆ—ï¼Œé¿å… PyArrow æŠ¥é”™
    if "pattern" in glossary_df.columns:
        glossary_df = glossary_df.drop(columns=["pattern"]) 

    st.dataframe(glossary_df, use_container_width=True, height=220)

    # âœï¸ æœ¯è¯­è¡¨ç¼–è¾‘
    with st.expander("âœï¸ ç¼–è¾‘æœ¯è¯­è¡¨ï¼ˆå¯å¢åˆ æ”¹ï¼‰", expanded=False):
        edited_gloss = st.data_editor(
            glossary_df,
            num_rows="dynamic",
            use_container_width=True,
            height=280,
            column_config={"source": "è‹±æ–‡åŸæ–‡", "target": "ä¸­æ–‡è¯‘å", "type": "ç±»åˆ«", "note": "å¤‡æ³¨"},
        )
        c1, c2, c3 = st.columns(3)
        if c1.button("ä¿å­˜æœ¯è¯­è¡¨æ›´æ”¹åˆ°ä¼šè¯"):
            # ä¿å­˜å‰å»æ‰ç¼–è¯‘åçš„æ­£åˆ™åˆ—
            if "pattern" in edited_gloss.columns:
                edited_gloss = edited_gloss.drop(columns=["pattern"]) 
            st.session_state["glossary_df"] = edited_gloss
            st.success("å·²ä¿å­˜åˆ°ä¼šè¯ã€‚åç»­ Tabs å°†ä½¿ç”¨æ›´æ–°åçš„æœ¯è¯­è¡¨ã€‚")
        c2.download_button("ä¸‹è½½å½“å‰æœ¯è¯­è¡¨ CSV", edited_gloss.to_csv(index=False).encode("utf-8"), "glossary_edited.csv")
        c3.caption("å»ºè®®å­—æ®µï¼šsource,target,type,noteï¼›type å¸¸è§å€¼ï¼šship/org/rank/role/species/place/tech")

    if 'chapter_text' not in locals() or not chapter_text.strip():
        st.info("è¯·å…ˆåœ¨ Tab1 ç²˜è´´æ–‡æœ¬å¹¶ä¼°ä»·ï¼Œä»¥ä¾¿è¿›è¡Œå‘½ä¸­åˆ†æã€‚")
    else:
        glossary_current = st.session_state.get("glossary_df", glossary_df)
        # æ„å»ºåŒ¹é…è¡¨æ—¶ä½¿ç”¨æ—  pattern çš„å‰¯æœ¬
        glossary_slim = glossary_current.drop(columns=["pattern"], errors='ignore')
        gdf = build_patterns(glossary_slim)
        hits = find_hits(chapter_text, gdf)
        c1, c2 = st.columns(2)
        with c1:
            st.write("**æœ¯è¯­å‘½ä¸­**")
            if hits:
                hit_df = pd.DataFrame([h.__dict__ for h in hits])
                st.dataframe(hit_df, use_container_width=True, height=260)
                st.download_button("ä¸‹è½½å‘½ä¸­æŠ¥å‘Š CSV", hit_df.to_csv(index=False).encode("utf-8"), "glossary_hits.csv")
            else:
                st.caption("æœªæ£€æµ‹åˆ°æœ¯è¯­å‘½ä¸­ã€‚")
        with c2:
            st.write("**ä¸ç¿»è¯‘äººåï¼ˆæç¤ºè¯è‡ªæ£€æµ‹ï¼‰**")
            st.caption("æœ¬æ¨¡å¼ä¸å†å±•ç¤º/ç¼–è¾‘äººåæ¸…å•ï¼Œæ¨¡å‹å°†ä¾æ®ç³»ç»Ÿæç¤ºè‡ªåŠ¨è¯†åˆ«äººåå¹¶ä¿ç•™è‹±æ–‡åŸæ ·ã€‚")

# --- Tab3: è§„åˆ™ä¸ç³»ç»Ÿæç¤º + å•æ¬¡è°ƒç”¨ ---
with T3:
    st.subheader("ç¿»è¯‘è§„åˆ™ (YAML)")
    rules_path = Path("data/rules_sample.yaml")
    default_yaml = rules_path.read_text(encoding="utf-8") if rules_path.exists() else None
    rules_text = st.text_area("ç¼–è¾‘/ç²˜è´´è§„åˆ™ YAMLï¼š", value=default_yaml, height=240)
    rules = parse_rules_yaml(rules_text)
    st.caption("è¿™äº›è§„åˆ™å°†æ³¨å…¥ç³»ç»Ÿæç¤ºï¼Œå¼ºåˆ¶äººåä¸ç¿»ã€èˆ°å/å†›è¡”æ˜ å°„ã€æœ¯è¯­ä¸€è‡´ç­‰ã€‚")

    if 'chapter_text' in locals() and chapter_text.strip():
        try:
            gdf
        except NameError:
            glossary_current = st.session_state.get("glossary_df", glossary_df)
            glossary_slim = glossary_current.drop(columns=["pattern"], errors='ignore')
            gdf = build_patterns(glossary_slim)
        hits = find_hits(chapter_text, gdf)
        hit_terms = sorted({h.term for h in hits})
        glossary_current = st.session_state.get("glossary_df", glossary_df)
        glossary_slim = glossary_current.drop(columns=["pattern"], errors='ignore')
        glossary_subset = glossary_slim[glossary_slim['source'].isin(hit_terms)].copy() if hit_terms else glossary_slim.head(20).copy()

        # äººååˆ—è¡¨ï¼šæç¤ºè¯è‡ªæ£€æµ‹ï¼Œä¸å†ä¼ åå•
        names = []

        sys_prompt = build_system_prompt(rules, glossary_subset, names)
        st.markdown("**ç³»ç»Ÿæç¤ºï¼ˆç”¨äºè°ƒç”¨ç¿»è¯‘ï¼‰**")
        st.code(sys_prompt, language="json")

        # è½»é‡ QC
        rep = quick_qc(len(hits), len(names))
        with st.expander("å¿«é€Ÿè´¨é‡æ£€æŸ¥"):
            st.json({"glossary_hits": rep.glossary_hits, "names_detected": rep.names_detected, "violations": rep.violations})

        # å•æ¬¡è°ƒç”¨ï¼ˆå®é™…è°ƒç”¨ï¼‰
        temp = 1.0
        st.caption("temperature å·²å›ºå®šä¸º 1ï¼ˆè¯¥æ¨¡å‹ä»…æ”¯æŒé»˜è®¤å€¼ï¼‰ã€‚")
        max_toks = st.number_input("max_output_tokens(å¯é€‰)", value=0, min_value=0, step=50, help="0 è¡¨ç¤ºä¸é™åˆ¶")
        resp_fmt = st.selectbox("response_format", ["text", "json"], index=0, help="è‹¥é€‰ jsonï¼Œå°†å‘é€ JSON schema(ç®€åŒ–ç¤ºä¾‹)")
        response_format = None
        if resp_fmt == "json":
            response_format = {"type": "json_object"}

        disabled = not (api_key or os.getenv("OPENAI_API_KEY"))
        if st.button("è¯•è¿è¡Œç¿»è¯‘", type="secondary", disabled=disabled):
            adapter = Translator(model, temperature=temp, max_output_tokens=(None if max_toks==0 else max_toks), response_format=response_format, api_key=api_key)
            try:
                result = adapter.translate_once(system_prompt=sys_prompt, user_text=chapter_text[:4000])
                st.text_area("è¿”å›ç¤ºä¾‹", value=result.text, height=220)
                fname = st.text_input("å¯¼å‡ºæ–‡ä»¶å", value="translation.txt")
                st.download_button("ä¸‹è½½TXT", result.text.encode("utf-8"), file_name=fname, mime="text/plain")
                if result.meta.get("usage"):
                    st.caption(f"usage: {result.meta['usage']}")
            except Exception as e:
                st.error(f"è°ƒç”¨å¤±è´¥ï¼š{e}")
        elif disabled:
            st.info("è¯·åœ¨å·¦ä¾§è¾“å…¥ OpenAI API Key æˆ–è®¾ç½®ç¯å¢ƒå˜é‡åå†è¯•ã€‚")

# --- Tab4: æ‰¹å¤„ç† JSONL ---
with T4:
    st.subheader("æ‰¹å¤„ç†ï¼šä»æ–‡ä»¶å¤¹è¯»å–ç« èŠ‚ï¼Œç”Ÿæˆ Batch JSONL")
    colA, colB = st.columns([2,1])
    with colA:
        folder = st.text_input("ç« èŠ‚æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆè¯»å– *.txtï¼‰", value=str(Path.cwd() / "chapters"))
    with colB:
        out_jsonl = st.text_input("è¾“å‡º JSONL è·¯å¾„", value=str(Path.cwd() / "batch" / "requests.jsonl"))

    st.caption("æµç¨‹ï¼šè¯»å–æ¯ä¸ªç« èŠ‚ â†’ ç”Ÿæˆç³»ç»Ÿæç¤ºï¼ˆåŸºäºå„è‡ªå‘½ä¸­çš„æœ¯è¯­å­é›†ï¼›äººåç”±æç¤ºè¯è‡ªæ£€æµ‹ï¼‰â†’ ç»„è£…ä¸º /v1/chat/completions çš„ JSONL æ‰¹å¤„ç†æ–‡ä»¶ã€‚")

    if st.button("ç”Ÿæˆ JSONL"):
        chs = chapters_from_folder(folder)
        if not chs:
            st.error("æœªåœ¨è¯¥è·¯å¾„å‘ç° .txt ç« èŠ‚æ–‡ä»¶ã€‚")
        else:
            glossary_current = st.session_state.get("glossary_df", glossary_df)
            glossary_slim = glossary_current.drop(columns=["pattern"], errors='ignore')
            gdf = build_patterns(glossary_slim)
            rows = []
            for it in chs:
                text = it["text"]
                hits = find_hits(text, gdf)
                terms = sorted({h.term for h in hits})
                subset = glossary_slim[glossary_slim['source'].isin(terms)].copy() if terms else glossary_slim.head(20).copy()
                # äººåç”±æç¤ºè¯è‡ªæ£€æµ‹
                names = []

                system_prompt = build_system_prompt(rules, subset, names)
                rows.append({
                    "id": it["id"],
                    "system_prompt": system_prompt,
                    "user_text": text,
                })
            adapter = Translator(model, api_key=api_key)
            jsonl_rows = adapter.prepare_batch_items(rows)
            build_batch_jsonl(jsonl_rows, out_jsonl)
            st.success(f"å·²ç”Ÿæˆï¼š{out_jsonl}")
            p = Path(out_jsonl)
            if p.exists():
                st.code(p.read_text(encoding='utf-8')[:1200] + "...", language="json")