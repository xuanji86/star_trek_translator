import os
import math
from pathlib import Path

import pandas as pd
import streamlit as st
import re

from core.tokenizer import estimate_tokens
from core.pricing import DEFAULT_PRICING, estimate_cost
from core.glossary import (
    load_glossary_csv, build_patterns, find_hits,
    normalize_glossary_df, merge_corpora,
)
from core.rules import parse_rules_yaml
from core.prompts import build_system_prompt
from core.qc import quick_qc
from core.translator import Translator
from core.batching import chapters_from_folder, build_batch_jsonl

# ===== åˆ†æ®µç¿»è¯‘è¾…åŠ©å‡½æ•° =====
def _normalize_newlines(text: str) -> str:
    # ç»Ÿä¸€æ¢è¡Œï¼Œé¿å… Windows/Mac ä¸åŒæ¢è¡Œå¯¼è‡´åˆ†å‰²å¼‚å¸¸
    return text.replace("\r\n", "\n").replace("\r", "\n")

def _safe_paragraphs(text: str) -> list[str]:
    """
    ä¼˜å…ˆæŒ‰â€œâ‰¥2ä¸ªæ¢è¡Œâ€åˆ‡æ®µï¼›è‹¥åˆ‡ä¸åŠ¨ï¼Œå†é€€åŒ–ä¸ºæŒ‰å¥å·/é—®å·/æ„Ÿå¹å·åˆ‡å¥ï¼›
    å†ä¸è¡Œï¼Œæœ€åæŒ‰å›ºå®šé•¿åº¦å…œåº•ï¼Œé¿å…ä»»ä½•ç©ºåˆ†éš”ç¬¦é”™è¯¯ã€‚
    """
    t = _normalize_newlines(text)
    paras = [p.strip() for p in re.split(r"\n{2,}", t) if p and p.strip()]
    if paras:
        return paras

    # æ²¡æœ‰ç©ºè¡Œå°±æŒ‰å¥å­åˆ‡
    sents = [s.strip() for s in re.split(r"(?<=[.!?])\s+", t) if s and s.strip()]
    if sents:
        return sents

    # ä»ç„¶åˆ‡ä¸å‡ºæ¥ï¼ˆæç«¯é•¿æ®µï¼‰ï¼ŒæŒ‰å›ºå®šå­—ç¬¦å®½åº¦å…œåº•
    CHUNK = 1200
    t = t.strip()
    if not t:
        return []
    return [t[i:i+CHUNK] for i in range(0, len(t), CHUNK)]

def split_text_by_tokens(text: str, model: str, max_input_tokens: int = 6000):
    """
    è¿‘ä¼¼æŒ‰ tokens åˆ‡åˆ†ï¼šä»¥â€œæ®µ/å¥/å®šé•¿å…œåº•â€ä¸ºå•ä½ç´¯ç§¯ï¼Œè¶…è¿‡é¢„ç®—å°±åˆ‡å—ã€‚
    - max_input_tokens è¦ä¸ºç³»ç»Ÿæç¤ºä¸è¾“å‡ºç•™ä½™é‡ï¼›<1000 æ—¶è‡ªåŠ¨æŠ¬åˆ° 1000 ä»¥é˜²æç«¯è®¾ç½®ã€‚
    """
    if not isinstance(text, str) or not text.strip():
        return
    budget = max(1000, int(max_input_tokens or 6000))

    units = _safe_paragraphs(text)
    buf, buf_tokens = [], 0
    for u in units:
        t = estimate_tokens(u, model)
        # å•ä¸ª unit è¶…è¿‡é¢„ç®—ï¼šç›´æ¥ä½œä¸ºç‹¬ç«‹å—ï¼ˆé¿å…æ­»å¾ªç¯ï¼‰
        if t >= budget:
            if buf:
                yield "\n\n".join(buf)
                buf, buf_tokens = [], 0
            yield u
            continue

        # æ­£å¸¸ç´¯ç§¯
        if buf and buf_tokens + t > budget:
            yield "\n\n".join(buf)
            buf, buf_tokens = [u], t
        else:
            buf.append(u)
            buf_tokens += t

    if buf:
        yield "\n\n".join(buf)

def translate_full_text(adapter, system_prompt: str, text: str, model: str,
                        max_input_tokens: int = 6000) -> str:
    """
    åˆ†æ®µç¿»è¯‘æ•´ç« å¹¶æ‹¼æ¥ï¼›å¤±è´¥æ—¶æŠ›å¼‚å¸¸ï¼Œç”±ä¸Šå±‚ UI æ•è·ã€‚
    """
    chunks = list(split_text_by_tokens(text, model, max_input_tokens=max_input_tokens))
    if not chunks:
        return ""
    outputs = []
    for i, ck in enumerate(chunks, 1):
        st.info(f"ç¿»è¯‘åˆ†æ®µ {i}/{len(chunks)}â€¦")
        res = adapter.translate_once(system_prompt=system_prompt, user_text=ck)
        outputs.append((res.text or "").strip())
    return "\n\n".join([o for o in outputs if o]).strip()

st.set_page_config(page_title="Star Trek ç¿»è¯‘åŠ©æ‰‹ Â· GUI & Batch", layout="wide")

# ===== Sidebar: åŸºæœ¬è®¾ç½® =====
st.sidebar.header("è®¾ç½®")
model = st.sidebar.selectbox("é€‰æ‹©æ¨¡å‹", ["gpt-5-mini", "gpt-5"], index=0)
batch_mode = st.sidebar.checkbox("Batch APIï¼ˆçº¦ -20% æˆæœ¬ï¼‰", value=True)
batch_discount = 0.20 if batch_mode else 0.0

# ä»…ä¼šè¯å†…å­˜å‚¨ï¼Œä¸è½ç›˜
api_key = st.sidebar.text_input(
    "OpenAI API Key", type="password", help="ä»…å½“å‰ä¼šè¯ä½¿ç”¨ï¼›ç•™ç©ºåˆ™å°è¯•ä½¿ç”¨ç¯å¢ƒå˜é‡ OPENAI_API_KEY"
)

# è®¡ä»·è®¾ç½®
pricing = {}
for m in ["gpt-5-mini", "gpt-5"]:
    col1, col2 = st.sidebar.columns(2)
    with col1:
        pi = st.number_input(
            f"{m} è¾“å…¥($/1M)",
            value=float(DEFAULT_PRICING[m]["input"]),
            min_value=0.0,
            step=0.05,
            key=f"pi_{m}",
        )
    with col2:
        po = st.number_input(
            f"{m} è¾“å‡º($/1M)",
            value=float(DEFAULT_PRICING[m]["output"]),
            min_value=0.0,
            step=0.10,
            key=f"po_{m}",
        )
    pricing[m] = {"input": pi, "output": po}

out_multiplier = st.sidebar.slider("è¾“å‡ºtoken/è¾“å…¥æ¯”ä¾‹", 1.05, 1.30, 1.10, 0.01)

st.title("ğŸ–– æ˜Ÿé™…è¿·èˆªç¿»è¯‘åŠ©æ‰‹ï¼šå•ç«  GUI + æ‰¹é‡è„šæ‰‹æ¶ï¼ˆå¤šè¯­æ–™åº“ç‰ˆï¼‰")

# ===== Tabs =====
T1, T2, T3, T4 = st.tabs([
    "â‘  ç²˜è´´æ•´ç« ä¼°ä»·",
    "â‘¡ æœ¯è¯­/å¤šè¯­æ–™åº“ç®¡ç†",
    "â‘¢ è§„åˆ™ä¸ç³»ç»Ÿæç¤º",
    "â‘£ æ‰¹å¤„ç†/JSONL ç”Ÿæˆ",
])

# --- Tab1: ä¼°ä»· ---
with T1:
    st.subheader("æ•´ç« è‹±æ–‡å†…å®¹")
    chapter_text = st.text_area("ç²˜è´´è‹±æ–‡åŸæ–‡ï¼š", height=300, placeholder="Paste chapter text hereâ€¦")
    if st.button("è®¡ç®—æˆæœ¬", type="primary"):
        if not chapter_text.strip():
            st.warning("è¯·å…ˆç²˜è´´æ–‡æœ¬ã€‚")
        else:
            in_tokens = estimate_tokens(chapter_text, model)
            out_tokens = math.ceil(in_tokens * out_multiplier)
            cb = estimate_cost(model, in_tokens, out_tokens, pricing, batch_discount)
            c1, c2, c3 = st.columns(3)
            c1.metric("è¾“å…¥ tokens", f"{cb.input_tokens:,}")
            c2.metric("è¾“å‡º tokens(ä¼°)", f"{cb.output_tokens:,}")
            c3.metric("é¢„è®¡æˆæœ¬(USD)", f"{cb.total_cost:.2f}")
            st.caption(f"æ¨¡å‹ï¼š{model} Â· Batchï¼š{'ON' if batch_mode else 'OFF'} Â· è¾“å‡ºå€ç‡ï¼š{out_multiplier:.2f}")

# --- Tab2: å¤šè¯­æ–™åº“ç®¡ç† ---
with T2:
    st.subheader("å¤šè¯­æ–™åº“ï¼ˆå¯æ‰©å±•ï¼‰ï¼šèˆ°èˆ¹/ç‰©ç§/èŒä½/ç‰©å“â€¦")

    # åˆå§‹åŒ–ä¼šè¯æ€ï¼š{'base': df, 'ships': df, ...}
    if "corpora" not in st.session_state:
        sample_path = Path("data/glossary_sample.csv")
        fallback_df = (
            pd.read_csv(sample_path)
            if sample_path.exists()
            else pd.DataFrame([
                {"source":"U.S.S. Enterprise","target":"è”é‚¦æ˜Ÿèˆ°ä¼ä¸šå·","type":"ship","note":"èˆ°åç¿»è¯‘"},
                {"source":"Enterprise","target":"ä¼ä¸šå·","type":"ship","note":"èˆ°åç¿»è¯‘"},
                {"source":"U.S.S. Titan","target":"è”é‚¦æ˜Ÿèˆ°æ³°å¦å·","type":"ship","note":"èˆ°åç¿»è¯‘"},
                {"source":"Titan","target":"æ³°å¦å·","type":"ship","note":"èˆ°åç¿»è¯‘"},
                {"source":"U.S.S. Aventine","target":"è”é‚¦æ˜Ÿèˆ°å®‰æ–‡å©·å·","type":"ship","note":"èˆ°åç¿»è¯‘"},
                {"source":"Aventine","target":"å®‰æ–‡å©·å·","type":"ship","note":"èˆ°åç¿»è¯‘"},
                {"source":"Borg","target":"åšæ ¼","type":"species","note":"ç‰©ç§"},
                {"source":"Borg drone","target":"åšæ ¼ä¸ªä½“","type":"species","note":"ä¸ªä½“ç§°è°“ï¼ˆæŒ‰ç‰©ç§å½’ç±»ï¼‰"},
                {"source":"Starfleet","target":"æ˜Ÿé™…èˆ°é˜Ÿ","type":"org","note":""},
                {"source":"Captain","target":"ä¸Šæ ¡","type":"rank","note":""},
                {"source":"Commander","target":"ä¸­æ ¡","type":"rank","note":""},
                {"source":"Lieutenant Commander","target":"å°‘æ ¡","type":"rank","note":""},
                {"source":"Operations manager","target":"æ“ä½œå®˜","type":"role","note":""},
                {"source":"Security chief","target":"å®‰å…¨å®˜","type":"role","note":""},
                {"source":"Flight controller","target":"èˆµæ‰‹","type":"role","note":""},
                {"source":"Number One","target":"å¤§å‰¯","type":"role","note":""},
                {"source":"Chief engineer","target":"è½®æœºé•¿","type":"role","note":""},
                {"source":"turbolift","target":"æ¶¡è½®ç”µæ¢¯","type":"item","note":""},
            ])
        )
        st.session_state["corpora"] = {
            "base": normalize_glossary_df(fallback_df, corpus_name="base")
        }

    corpora = st.session_state["corpora"]

    # ä»ç›®å½•æ‰¹é‡å¯¼å…¥ *.csv ä¸ºå¤šä¸ªè¯­æ–™åº“
    st.markdown("**ä»ç›®å½•æ‰¹é‡å¯¼å…¥ CSV**ï¼ˆæ¯ä¸ª CSV è§†ä¸ºä¸€ä¸ªè¯­æ–™åº“ï¼Œæ–‡ä»¶åä¸ºè¯­æ–™åº“åï¼‰")
    colA, colB = st.columns([2, 1])
    with colA:
        corpora_dir = st.text_input("è¯­æ–™åº“ç›®å½•", value=str(Path.cwd() / "data/corpora"))
    with colB:
        if st.button("æ‰«æå¹¶å¯¼å…¥ç›®å½•"):
            p = Path(corpora_dir)
            if p.exists() and p.is_dir():
                count = 0
                for f in sorted(p.glob("*.csv")):
                    try:
                        df = pd.read_csv(f)
                        df = normalize_glossary_df(df, corpus_name=f.stem)
                        corpora[f.stem] = df
                        count += 1
                    except Exception as e:
                        st.warning(f"è·³è¿‡ {f.name}: {e}")
                st.success(f"å·²å¯¼å…¥ {count} ä¸ªè¯­æ–™åº“ã€‚")
            else:
                st.error("ç›®å½•ä¸å­˜åœ¨ã€‚")

    # å•æ–‡ä»¶æ–°å¢è¯­æ–™åº“ï¼ˆå¯å¤šæ¬¡ä¸Šä¼ ï¼‰
    st.markdown("**ä¸Šä¼  CSV æ–°å¢è¯­æ–™åº“**ï¼ˆå­—æ®µ: source,target,type,noteï¼‰")
    up_files = st.file_uploader("é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ª CSV", type=["csv"], accept_multiple_files=True)
    if up_files:
        for uf in up_files:
            try:
                df = pd.read_csv(uf)
                df = normalize_glossary_df(df, corpus_name=Path(uf.name).stem)
                corpora[Path(uf.name).stem] = df
            except Exception as e:
                st.warning(f"è·³è¿‡ {uf.name}: {e}")
        st.success(f"å·²æ·»åŠ  {len(up_files)} ä¸ªè¯­æ–™åº“åˆ°ä¼šè¯ã€‚")

    # æ–°å»ºç©ºç™½è¯­æ–™åº“
    with st.expander("â• æ–°å»ºç©ºç™½è¯­æ–™åº“", expanded=False):
        new_name = st.text_input("è¯­æ–™åº“åç§°", placeholder="ä¾‹å¦‚ ships/species/roles/items æˆ–ä»»æ„è‡ªå®šä¹‰")
        if st.button("åˆ›å»ºç©ºç™½è¯­æ–™åº“"):
            if not new_name.strip():
                st.warning("è¯·è¾“å…¥åç§°ã€‚")
            elif new_name in corpora:
                st.warning("è¯¥åç§°å·²å­˜åœ¨ã€‚")
            else:
                corpora[new_name] = normalize_glossary_df(
                    pd.DataFrame(columns=["source", "target", "type", "note"]),
                    corpus_name=new_name,
                )
                st.success(f"å·²åˆ›å»ºï¼š{new_name}")

    # é€‰æ‹©å¹¶ç¼–è¾‘æŸä¸ªè¯­æ–™åº“
    st.markdown("**ç¼–è¾‘è¯­æ–™åº“**")
    corpus_names = sorted(corpora.keys())
    sel = st.selectbox("é€‰æ‹©è¯­æ–™åº“", corpus_names, index=corpus_names.index("base") if "base" in corpus_names else 0)
    cur_df = corpora[sel].copy()
    cur_df = cur_df.drop(columns=["pattern"], errors="ignore")  # å±•ç¤ºæ—¶å»æ‰ç¼–è¯‘åˆ—
    st.dataframe(cur_df, use_container_width=True, height=240)

    with st.expander("âœï¸ å°±åœ°ç¼–è¾‘å¹¶ä¿å­˜", expanded=False):
        edited = st.data_editor(
            cur_df,
            num_rows="dynamic",
            use_container_width=True,
            height=300,
            column_config={"source":"è‹±æ–‡", "target":"ä¸­æ–‡", "type":"ç±»åˆ«", "note":"å¤‡æ³¨", "corpus":"è¯­æ–™åº“"},
        )
        c1, c2, c3, c4 = st.columns(4)
        if c1.button("ä¿å­˜åˆ°å½“å‰è¯­æ–™åº“"):
            edited = normalize_glossary_df(edited, corpus_name=sel)
            corpora[sel] = edited
            st.success("å·²ä¿å­˜åˆ°ä¼šè¯ã€‚")
        if c2.button("ä¸‹è½½å½“å‰è¯­æ–™åº“ CSV"):
            st.download_button("ç‚¹æ­¤ä¸‹è½½", edited.to_csv(index=False).encode("utf-8"), file_name=f"{sel}.csv")
        if c3.button("åˆ é™¤è¯¥è¯­æ–™åº“"):
            if sel == "base":
                st.warning("åŸºç¡€è¯­æ–™åº“ base ä¸å»ºè®®åˆ é™¤ã€‚")
            else:
                del corpora[sel]
                st.experimental_rerun()
        with c4:
            st.caption("type å¸¸è§: ship/species/role/rank/item/org/place/techâ€¦")

    # åˆå¹¶è§†å›¾
    merged = merge_corpora(corpora)
    st.markdown("**åˆå¹¶æ€»è§ˆï¼ˆä»…å±•ç¤ºï¼Œä¸å«ç¼–è¯‘åˆ—ï¼‰**")
    st.dataframe(merged.drop(columns=["pattern"], errors="ignore"), use_container_width=True, height=260)

    # å‘½ä¸­åˆ†æï¼ˆåŸºäºåˆå¹¶è¯­æ–™åº“ï¼‰
    if 'chapter_text' not in locals() or not chapter_text.strip():
        st.info("Tab1 ç²˜è´´æ–‡æœ¬åï¼Œè¿™é‡Œå¯åšæœ¯è¯­å‘½ä¸­åˆ†æã€‚")
    else:
        gdf = build_patterns(merged.drop(columns=["pattern"], errors="ignore"))
        hits = find_hits(chapter_text, gdf)
        if hits:
            hit_df = pd.DataFrame([h.__dict__ for h in hits])
            st.dataframe(hit_df, use_container_width=True, height=240)
            st.download_button("ä¸‹è½½å‘½ä¸­æŠ¥å‘Š CSV", hit_df.to_csv(index=False).encode("utf-8"), "glossary_hits.csv")
        else:
            st.caption("æœªæ£€æµ‹åˆ°æœ¯è¯­å‘½ä¸­ã€‚")

# --- Tab3: è§„åˆ™ä¸ç³»ç»Ÿæç¤º + å•æ¬¡è°ƒç”¨ ---
with T3:
    st.subheader("ç¿»è¯‘è§„åˆ™ (YAML)")
    rules_path = Path("data/rules_sample.yaml")
    default_yaml = rules_path.read_text(encoding="utf-8") if rules_path.exists() else None
    rules_text = st.text_area("ç¼–è¾‘/ç²˜è´´è§„åˆ™ YAMLï¼š", value=default_yaml, height=240)
    rules = parse_rules_yaml(rules_text)
    st.caption("è¿™äº›è§„åˆ™å°†æ³¨å…¥ç³»ç»Ÿæç¤ºï¼Œå¼ºåˆ¶äººåä¸ç¿»ã€èˆ°å/å†›è¡”/èŒä½/ç‰©ç§/ç‰©å“ç­‰ç»Ÿä¸€è¯‘åã€‚")

    if 'chapter_text' in locals() and chapter_text.strip():
        corpora = st.session_state.get('corpora', {})
        merged = merge_corpora(corpora)
        gdf = build_patterns(merged.drop(columns=["pattern"], errors="ignore"))
        hits = find_hits(chapter_text, gdf)
        hit_terms = sorted({h.term for h in hits})
        glossary_subset = merged[merged['source'].isin(hit_terms)].copy() if hit_terms else merged.head(50).copy()

        # äººåç”±æç¤ºè¯è‡ªæ£€æµ‹ï¼Œä¸å†ä¼ åå•
        names = []

        sys_prompt = build_system_prompt(rules, glossary_subset.drop(columns=["pattern"], errors='ignore'), names)
        st.markdown("**ç³»ç»Ÿæç¤ºï¼ˆç”¨äºè°ƒç”¨ç¿»è¯‘ï¼‰**")
        st.code(sys_prompt, language="json")

        # è½»é‡ QC
        rep = quick_qc(len(hits), len(names))
        with st.expander("å¿«é€Ÿè´¨é‡æ£€æŸ¥"):
            st.json({"glossary_hits": rep.glossary_hits, "names_detected": rep.names_detected, "violations": rep.violations})

        # å•æ¬¡è°ƒç”¨ï¼ˆå®é™…è°ƒç”¨ï¼‰
        temp = 1.0
        st.caption("temperature å·²å›ºå®šä¸º 1ï¼ˆè¯¥æ¨¡å‹ä»…æ”¯æŒé»˜è®¤å€¼ï¼‰ã€‚")
        max_toks = st.number_input("max_output_tokens(å¯é€‰)", value=0, min_value=0, step=50, help="0 è¡¨ç¤ºä¸é™åˆ¶")
        resp_fmt = st.selectbox("response_format", ["text", "json"], index=0, help="è‹¥é€‰ jsonï¼Œå°†å‘é€ JSON schema(ç®€åŒ–ç¤ºä¾‹)")
        response_format = None
        if resp_fmt == "json":
            response_format = {"type": "json_object"}

        disabled = not (api_key or os.getenv("OPENAI_API_KEY"))
        if st.button("è¯•è¿è¡Œç¿»è¯‘", type="secondary", disabled=disabled):
            adapter = Translator(model, temperature=temp, max_output_tokens=(None if max_toks==0 else max_toks), response_format=response_format, api_key=api_key)
            try:
                result = adapter.translate_once(system_prompt=sys_prompt, user_text=chapter_text[:4000])
                st.text_area("è¿”å›ç¤ºä¾‹", value=result.text, height=220)
                fname = st.text_input("å¯¼å‡ºæ–‡ä»¶å", value="translation.txt")
                st.download_button("ä¸‹è½½TXT", result.text.encode("utf-8"), file_name=fname, mime="text/plain")
                if result.meta.get("usage"):
                    st.caption(f"usage: {result.meta['usage']}")
            except Exception as e:
                st.error(f"è°ƒç”¨å¤±è´¥ï¼š{e}")
        elif disabled:
            st.info("è¯·åœ¨å·¦ä¾§è¾“å…¥ OpenAI API Key æˆ–è®¾ç½®ç¯å¢ƒå˜é‡åå†è¯•ã€‚")

        # â€”â€” æ•´ç« ç¿»è¯‘ï¼ˆè‡ªåŠ¨åˆ†æ®µï¼‰ â€”â€”
        st.markdown("---")
        st.subheader("æ•´ç« ç¿»è¯‘ï¼ˆè‡ªåŠ¨åˆ†æ®µï¼‰")
        st.caption("æŒ‰ token é¢„ç®—è‡ªåŠ¨åˆ‡å—ï¼Œé€æ®µè°ƒç”¨å¹¶åˆå¹¶ä¸ºå…¨æ–‡ï¼›é€‚ç”¨äºé•¿ç« èŠ‚/æ•´ç« ã€‚")
        max_in_budget = st.number_input(
            "æ¯æ®µæœ€å¤§è¾“å…¥ tokensï¼ˆä¸ºç³»ç»Ÿæç¤ºä¸è¾“å‡ºç•™ä½™é‡ï¼‰",
            value=6000, min_value=2000, max_value=24000, step=500
        )
        full_disabled = disabled or (not chapter_text.strip())
        if st.button("å¼€å§‹æ•´ç« ç¿»è¯‘", type="primary", disabled=full_disabled):
            adapter = Translator(
                model,
                temperature=1.0,
                max_output_tokens=(None if max_toks == 0 else max_toks),
                response_format=response_format,
                api_key=api_key,
            )
            try:
                with st.spinner("æ•´ç« ç¿»è¯‘è¿›è¡Œä¸­â€¦"):
                    full_text = translate_full_text(
                        adapter, sys_prompt, chapter_text, model,
                        max_input_tokens=int(max_in_budget)
                    )
                st.success("æ•´ç« ç¿»è¯‘å®Œæˆ âœ…")
                st.text_area("å…¨æ–‡è¯‘æ–‡ï¼ˆé¢„è§ˆï¼‰", value=full_text, height=320)
                fname_full = st.text_input("å¯¼å‡ºæ–‡ä»¶åï¼ˆå…¨æ–‡ï¼‰", value="chapter_translation_full.txt", key="full_fn")
                st.download_button("ä¸‹è½½å…¨æ–‡ TXT", full_text.encode("utf-8"), file_name=fname_full, mime="text/plain")
            except Exception as e:
                st.error(f"æ•´ç« ç¿»è¯‘å¤±è´¥ï¼š{e}")

# --- Tab4: æ‰¹å¤„ç† JSONL ---
with T4:
    st.subheader("æ‰¹å¤„ç†ï¼šä»æ–‡ä»¶å¤¹è¯»å–ç« èŠ‚ï¼Œç”Ÿæˆ Batch JSONL")
    colA, colB = st.columns([2, 1])
    with colA:
        folder = st.text_input("ç« èŠ‚æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆè¯»å– *.txtï¼‰", value=str(Path.cwd() / "chapters"))
    with colB:
        out_jsonl = st.text_input("è¾“å‡º JSONL è·¯å¾„", value=str(Path.cwd() / "batch" / "requests.jsonl"))

    st.caption("æµç¨‹ï¼šè¯»å–æ¯ä¸ªç« èŠ‚ â†’ åˆå¹¶è¯­æ–™åº“ â†’ ç”Ÿæˆç³»ç»Ÿæç¤ºï¼ˆæŒ‰å‘½ä¸­å­é›†ï¼›äººåç”±æç¤ºè¯è‡ªæ£€æµ‹ï¼‰â†’ ç»„è£…ä¸º /v1/chat/completions çš„ JSONL æ‰¹å¤„ç†æ–‡ä»¶ã€‚")

    if st.button("ç”Ÿæˆ JSONL"):
        chs = chapters_from_folder(folder)
        if not chs:
            st.error("æœªåœ¨è¯¥è·¯å¾„å‘ç° .txt ç« èŠ‚æ–‡ä»¶ã€‚")
        else:
            corpora = st.session_state.get('corpora', {})
            merged = merge_corpora(corpora)
            gdf = build_patterns(merged.drop(columns=["pattern"], errors="ignore"))
            rows = []
            for it in chs:
                text = it["text"]
                hits = find_hits(text, gdf)
                terms = sorted({h.term for h in hits})
                subset = merged[merged['source'].isin(terms)].copy() if terms else merged.head(50).copy()
                names = []  # äººåç”±æç¤ºè¯è‡ªæ£€æµ‹

                system_prompt = build_system_prompt(rules, subset.drop(columns=["pattern"], errors="ignore"), names)
                rows.append({
                    "id": it["id"],
                    "system_prompt": system_prompt,
                    "user_text": text,
                })
            adapter = Translator(model, api_key=api_key)
            jsonl_rows = adapter.prepare_batch_items(rows)
            build_batch_jsonl(jsonl_rows, out_jsonl)
            st.success(f"å·²ç”Ÿæˆï¼š{out_jsonl}")
            p = Path(out_jsonl)
            if p.exists():
                st.code(p.read_text(encoding='utf-8')[:1200] + "...", language="json")