import os
import io
import re
import math
import zipfile
from pathlib import Path

import pandas as pd
import streamlit as st
from openai import OpenAI
from typing import Optional, Dict, List, Tuple  # æ·»åŠ  Tuple

from core.tokenizer import estimate_tokens
from core.pricing import DEFAULT_PRICING, estimate_cost
from core.glossary import (
    load_glossary_csv,
    build_patterns,
    find_hits,
    normalize_glossary_df,
    merge_corpora,
)
from core.rules import parse_rules_yaml
from core.prompts import build_system_prompt
from core.qc import quick_qc
from core.translator import Translator
from core.batching import chapters_from_folder, build_batch_jsonl

# =============================
# åˆ†æ®µç¿»è¯‘è¾…åŠ©å‡½æ•°
# =============================

def _normalize_newlines(text: str) -> str:
    return text.replace("\r\n", "\n").replace("\r", "\n")

def _safe_paragraphs(text: str) -> List[str]:
    t = _normalize_newlines(text)
    paras = [p.strip() for p in re.split(r"\n{2,}", t) if p and p.strip()]
    if paras:
        return paras
    sents = [s.strip() for s in re.split(r"(?<=[.!?])\s+", t) if s and s.strip()]
    if sents:
        return sents
    CHUNK = 1200
    t = t.strip()
    if not t:
        return []
    return [t[i : i + CHUNK] for i in range(0, len(t), CHUNK)]

def split_text_by_tokens(text: str, model: str, max_input_tokens: int = 6000):
    if not isinstance(text, str) or not text.strip():
        return
    budget = max(1000, int(max_input_tokens or 6000))
    units = _safe_paragraphs(text)
    buf: List[str] = []
    buf_tokens = 0
    for u in units:
        t = estimate_tokens(u, model)
        if t >= budget:
            if buf:
                yield "\n\n".join(buf)
                buf, buf_tokens = [], 0
            yield u
            continue
        if buf and buf_tokens + t > budget:
            yield "\n\n".join(buf)
            buf, buf_tokens = [u], t
        else:
            buf.append(u)
            buf_tokens += t
    if buf:
        yield "\n\n".join(buf)

def translate_full_text(adapter: Translator, system_prompt: str, text: str, model: str,
                        max_input_tokens: int = 6000) -> str:
    chunks = list(split_text_by_tokens(text, model, max_input_tokens=max_input_tokens))
    if not chunks:
        return ""
    outputs: List[str] = []
    for i, ck in enumerate(chunks, 1):
        st.info(f"ç¿»è¯‘åˆ†æ®µ {i}/{len(chunks)}â€¦")
        res = adapter.translate_once(system_prompt=system_prompt, user_text=ck)
        outputs.append((res.text or "").strip())
    return "\n\n".join([o for o in outputs if o]).strip()

# =============================
# Batch API è¾…åŠ©å‡½æ•°
# =============================

def _make_client_from_gui(api_key: Optional[str]) -> OpenAI:
    if api_key and api_key.strip():
        return OpenAI(api_key=api_key.strip())
    return OpenAI()

def _save_results_jsonl_bytes(client: OpenAI, file_id: str, out_path: Path) -> None:
    content: bytes = client.files.content(file_id).content
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_bytes(content)

def _parse_results_jsonl(jsonl_path: Path) -> List[dict]:
    results: List[dict] = []
    if not jsonl_path.exists():
        return results
    for line in jsonl_path.read_text(encoding="utf-8").splitlines():
        if not line.strip():
            continue
        try:
            obj = __import__("json").loads(line)
        except Exception:
            continue
        cid = obj.get("custom_id")
        body = obj.get("response", {}).get("body", {})
        try:
            text = body["choices"][0]["message"]["content"]
        except Exception:
            text = ""
        results.append({"custom_id": cid, "text": text, "raw": body})
    return results

def _write_txt_outputs(results: List[dict], out_dir: Path) -> List[Path]:
    out_dir.mkdir(parents=True, exist_ok=True)
    paths: List[Path] = []
    for r in results:
        cid = (r.get("custom_id") or "unknown").replace("/", "_")
        p = out_dir / f"{cid}.txt"
        p.write_text(r.get("text") or "", encoding="utf-8")
        paths.append(p)
    return paths

def _zip_paths(paths: List[Path]) -> bytes:
    buf = io.BytesIO()
    with zipfile.ZipFile(buf, "w", zipfile.ZIP_DEFLATED) as zf:
        for p in paths:
            zf.write(p, arcname=p.name)
    buf.seek(0)
    return buf.read()

# =============================
# è´¨æ£€ä¸ä¿®å¤è¾…åŠ©å‡½æ•°
# =============================

def _merge_corpora_for_check(corpora: Dict[str, pd.DataFrame]) -> pd.DataFrame:
    try:
        return merge_corpora(corpora)
    except Exception:
        frames = []
        for name, df in (corpora or {}).items():
            if isinstance(df, pd.DataFrame) and not df.empty:
                tmp = df.copy()
                if "corpus" not in tmp.columns:
                    tmp["corpus"] = name
                frames.append(tmp)
        if not frames:
            return pd.DataFrame(columns=["source", "target", "type", "note", "corpus"])
        out = pd.concat(frames, ignore_index=True)
        out = out.drop_duplicates(subset=["source", "target", "type"], keep="first")
        return out

def _collect_rank_words(merged: pd.DataFrame) -> List[str]:
    if merged is None or merged.empty:
        return ["ä¸Šæ ¡", "ä¸­æ ¡", "å°‘æ ¡", "ä¸Šå°‰", "ä¸­å°‰", "å°‘å°‰", "èˆ°é•¿"]
    # åªå– type == rank çš„ target
    try:
        ranks = merged[merged["type"] == "rank"]["target"].dropna().astype(str).tolist()
    except Exception:
        ranks = []
    ranks = sorted({r.strip() for r in ranks if r.strip()}, key=len, reverse=True)
    return ranks or ["ä¸Šæ ¡", "ä¸­æ ¡", "å°‘æ ¡", "ä¸Šå°‰", "ä¸­å°‰", "å°‘å°‰", "èˆ°é•¿"]

def _normalize_paragraphs(txt: str) -> str:
    if not isinstance(txt, str):
        return ""
    t = txt.replace("\r\n", "\n").replace("\r", "\n")
    t = re.sub(r"\n{3,}", "\n\n", t)
    lines = [ln.rstrip() for ln in t.split("\n")]
    fixed: List[str] = []
    for i, ln in enumerate(lines):
        fixed.append(ln)
        if ln.strip() and i + 1 < len(lines) and lines[i + 1].strip():
            fixed.append("")
    t = "\n".join(fixed)
    t = t.strip("\n")
    t = re.sub(r"(\n)\s*(\n)", "\n\n", t)
    return t

def _find_rank_order_issues(translated: str, rank_words: List[str]) -> List[Tuple[str, str, int, int]]:
    """æ£€æµ‹åˆ° â€œå†›è¡” åœ¨å‰ã€åå­— åœ¨åâ€çš„è¿ä¾‹ï¼Œè¿”å› (rank, name, start, end)ã€‚"""
    if not translated:
        return []
    name_en = r"[A-Z][A-Za-z'\-]+(?:\s+[A-Z][A-Za-z'\-]+){0,2}"
    rank_regex = r"(?:" + "|".join(map(re.escape, rank_words)) + r")"
    pat_wrong = re.compile(rf"({rank_regex})\s*({name_en})")
    issues: List[Tuple[str, str, int, int]] = []
    for m in pat_wrong.finditer(translated):
        rank, name = m.group(1), m.group(2)
        issues.append((rank, name, m.start(), m.end()))
    return issues

def _auto_fix_rank_order(translated: str, rank_words: List[str]) -> str:
    if not translated:
        return translated
    name_en = r"[A-Z][A-Za-z'\-]+(?:\s+[A-Z][A-Za-z'\-]+){0,2}"
    rank_regex = r"(?:" + "|".join(map(re.escape, rank_words)) + r")"
    pat_wrong = re.compile(rf"\b({rank_regex})\s+({name_en})\b")
    return pat_wrong.sub(r"\2 \1", translated)

def _glossary_coverage(english: str, translated: str, merged: pd.DataFrame) -> pd.DataFrame:
    """
    åŸºäºè‹±æ–‡åŸæ–‡å‘½ä¸­é¡¹ï¼Œæ£€æŸ¥ï¼š
    - hit_in_english: è¯¥ source æ˜¯å¦å‡ºç°åœ¨è‹±æ–‡åŸæ–‡ä¸­
    - found_target_in_zh: ä»»ä¸€ target æ˜¯å¦å‡ºç°åœ¨è¯‘æ–‡ä¸­
    - found_source_in_zh: è‹±æ–‡ source æœ¬èº«æ˜¯å¦ä»æ®‹ç•™åœ¨è¯‘æ–‡ä¸­ï¼ˆç”¨äºè‡ªåŠ¨ä¿®å¤ï¼‰
    """
    if not english or merged is None or merged.empty:
        return pd.DataFrame(columns=[
            "source", "target", "type",
            "hit_in_english", "found_target_in_zh", "found_source_in_zh",
        ])

    # æ„å»ºè‹±æ–‡å‘½ä¸­
    gdf = build_patterns(merged.drop(columns=["pattern"], errors="ignore"))
    hits = find_hits(english, gdf) or []
    dedup = {}
    for h in hits:
        if h.term not in dedup:
            dedup[h.term] = h

    records = []
    zh = translated or ""
    for term, _h in dedup.items():
        rows = merged[merged["source"] == term]
        if rows.empty:
            continue

        targets = []
        for _, r in rows.iterrows():
            tgt = str(r.get("target") or "").strip()
            if tgt:
                targets.append(tgt)

        # è¯‘æ–‡ä¸­æ˜¯å¦å·²ç»æœ‰ä»»æ„ä¸€ä¸ª target
        any_target = False
        for tgt in targets:
            if tgt in zh:
                any_target = True
                break

        records.append({
            "source": term,
            "target": "; ".join(targets),
            "type": rows.iloc[0].get("type"),
            "hit_in_english": True,
            "found_target_in_zh": any_target,
            "found_source_in_zh": term in zh,
        })

    df = pd.DataFrame.from_records(records)
    if not df.empty:
        df = df.sort_values(
            ["found_target_in_zh", "type", "source"],
            ascending=[True, True, True]
        )
    return df

def _apply_glossary_repairs(translated: str, cov_df: pd.DataFrame) -> str:
    """
    æ ¹æ®è¦†ç›–æŠ¥å‘Šè‡ªåŠ¨ä¿®å¤ï¼š
    - æ¡ä»¶ï¼šhit_in_english=True ä¸” found_target_in_zh=False ä¸” found_source_in_zh=True
    - æ“ä½œï¼šæŠŠè¯‘æ–‡ä¸­çš„è‹±æ–‡ source æ›¿æ¢ä¸ºè¯­æ–™åº“ä¸­ç¬¬ä¸€ä¸ª target
    """
    if translated is None:
        return ""
    if cov_df is None or cov_df.empty:
        return translated

    text = translated
    for _, row in cov_df.iterrows():
        try:
            src = str(row.get("source") or "").strip()
            targets = [t.strip() for t in str(row.get("target") or "").split(";") if t.strip()]
            if not src or not targets:
                continue

            # åªå¯¹ã€Œè‹±æ–‡åŸæ–‡ä¸­å‡ºç°è¿‡ã€è¯‘æ–‡ä¸­è¿˜æ®‹ç•™ sourceã€ä¸”æ²¡æœ‰ targetã€çš„æ¡ç›®åšæ›¿æ¢
            if not row.get("hit_in_english"):
                continue
            if row.get("found_target_in_zh"):
                continue
            if not row.get("found_source_in_zh"):
                continue

            first_target = targets[0]
            # ç®€å•æŒ‰å­ä¸²æ›¿æ¢ï¼›å¦‚éœ€æ›´ä¸¥æ ¼å¯æ”¹æˆå¸¦è¯è¾¹ç•Œçš„æ­£åˆ™
            pattern = re.compile(re.escape(src))
            text = pattern.sub(first_target, text)
        except Exception:
            # é˜²å¾¡æ€§å…œåº•ï¼Œå•æ¡å¤±è´¥ä¸å½±å“æ•´ä½“
            continue

    return text
# =============================
# Streamlit UI
# =============================

st.set_page_config(page_title="Star Trek ç¿»è¯‘åŠ©æ‰‹ Â· GUI & Batch", layout="wide")

# Sidebar
st.sidebar.header("è®¾ç½®")
model = st.sidebar.selectbox("é€‰æ‹©æ¨¡å‹", ["gpt-5-mini", "gpt-5"], index=0)
batch_mode = st.sidebar.checkbox("Batch APIï¼ˆçº¦ -20% æˆæœ¬ï¼‰", value=True)
batch_discount = 0.20 if batch_mode else 0.0
api_key = st.sidebar.text_input(
    "OpenAI API Key", type="password", help="ä»…å½“å‰ä¼šè¯ä½¿ç”¨ï¼›ç•™ç©ºåˆ™å°è¯•ä½¿ç”¨ç¯å¢ƒå˜é‡ OPENAI_API_KEY"
)

pricing: Dict[str, Dict[str, float]] = {}
for m in ["gpt-5-mini", "gpt-5"]:
    col1, col2 = st.sidebar.columns(2)
    with col1:
        pi = st.number_input(f"{m} è¾“å…¥($/1M)", value=float(DEFAULT_PRICING[m]["input"]), min_value=0.0, step=0.05, key=f"pi_{m}")
    with col2:
        po = st.number_input(f"{m} è¾“å‡º($/1M)", value=float(DEFAULT_PRICING[m]["output"]), min_value=0.0, step=0.10, key=f"po_{m}")
    pricing[m] = {"input": pi, "output": po}

out_multiplier = st.sidebar.slider("è¾“å‡ºtoken/è¾“å…¥æ¯”ä¾‹", 1.05, 1.30, 1.10, 0.01)

st.title("ğŸ–– æ˜Ÿé™…è¿·èˆªç¿»è¯‘åŠ©æ‰‹ï¼šå•ç«  + æ‰¹é‡ + è´¨æ£€")

# Tabs
T1, T2, T3, T4, T5 = st.tabs([
    "â‘  ç²˜è´´æ•´ç« ä¼°ä»·",
    "â‘¡ æœ¯è¯­/å¤šè¯­æ–™åº“ç®¡ç†",
    "â‘¢ è§„åˆ™ä¸ç³»ç»Ÿæç¤º",
    "â‘£ æ‰¹å¤„ç†/JSONL ç”Ÿæˆ",
    "â‘¤ æˆå“è´¨æ£€/ä¿®å¤",
])

# Tab1: ä¼°ä»·
with T1:
    st.subheader("æ•´ç« è‹±æ–‡å†…å®¹")
    chapter_text = st.text_area("ç²˜è´´è‹±æ–‡åŸæ–‡ï¼š", height=300, placeholder="Paste chapter text hereâ€¦")
    if st.button("è®¡ç®—æˆæœ¬", type="primary"):
        if not chapter_text.strip():
            st.warning("è¯·å…ˆç²˜è´´æ–‡æœ¬ã€‚")
        else:
            in_tokens = estimate_tokens(chapter_text, model)
            out_tokens = math.ceil(in_tokens * out_multiplier)
            cb = estimate_cost(model, in_tokens, out_tokens, pricing, batch_discount)
            c1, c2, c3 = st.columns(3)
            c1.metric("è¾“å…¥ tokens", f"{cb.input_tokens:,}")
            c2.metric("è¾“å‡º tokens(ä¼°)", f"{cb.output_tokens:,}")
            c3.metric("é¢„è®¡æˆæœ¬(USD)", f"{cb.total_cost:.2f}")
            st.caption(f"æ¨¡å‹ï¼š{model} Â· Batchï¼š{'ON' if batch_mode else 'OFF'} Â· è¾“å‡ºå€ç‡ï¼š{out_multiplier:.2f}")

# Tab2: å¤šè¯­æ–™åº“ç®¡ç†
with T2:
    st.subheader("å¤šè¯­æ–™åº“ï¼ˆå¯æ‰©å±•ï¼‰ï¼šèˆ°èˆ¹/ç‰©ç§/èŒä½/ç‰©å“â€¦")

    if "corpora" not in st.session_state:
        sample_path = Path("data/glossary_sample.csv")
        fallback_df = (
            pd.read_csv(sample_path)
            if sample_path.exists()
            else pd.DataFrame([
                {"source": "U.S.S. Enterprise", "target": "è”é‚¦æ˜Ÿèˆ°ä¼ä¸šå·", "type": "ship", "note": "èˆ°åç¿»è¯‘"},
                {"source": "Enterprise", "target": "ä¼ä¸šå·", "type": "ship", "note": "èˆ°åç¿»è¯‘"},
                {"source": "U.S.S. Titan", "target": "è”é‚¦æ˜Ÿèˆ°æ³°å¦å·", "type": "ship", "note": "èˆ°åç¿»è¯‘"},
                {"source": "Titan", "target": "æ³°å¦å·", "type": "ship", "note": "èˆ°åç¿»è¯‘"},
                {"source": "U.S.S. Aventine", "target": "è”é‚¦æ˜Ÿèˆ°å®‰æ–‡å©·å·", "type": "ship", "note": "èˆ°åç¿»è¯‘"},
                {"source": "Aventine", "target": "å®‰æ–‡å©·å·", "type": "ship", "note": "èˆ°åç¿»è¯‘"},
                {"source": "Borg", "target": "åšæ ¼", "type": "species", "note": "ç‰©ç§"},
                {"source": "Borg drone", "target": "åšæ ¼ä¸ªä½“", "type": "species", "note": "ä¸ªä½“ç§°è°“ï¼ˆæŒ‰ç‰©ç§å½’ç±»ï¼‰"},
                {"source": "Starfleet", "target": "æ˜Ÿé™…èˆ°é˜Ÿ", "type": "org", "note": ""},
                {"source": "Captain", "target": "ä¸Šæ ¡", "type": "rank", "note": ""},
                {"source": "Commander", "target": "ä¸­æ ¡", "type": "rank", "note": ""},
                {"source": "Lieutenant Commander", "target": "å°‘æ ¡", "type": "rank", "note": ""},
                {"source": "Operations manager", "target": "æ“ä½œå®˜", "type": "role", "note": ""},
                {"source": "Security chief", "target": "å®‰å…¨å®˜", "type": "role", "note": ""},
                {"source": "Flight controller", "target": "èˆµæ‰‹", "type": "role", "note": ""},
                {"source": "Number One", "target": "å¤§å‰¯", "type": "role", "note": ""},
                {"source": "Chief engineer", "target": "è½®æœºé•¿", "type": "role", "note": ""},
                {"source": "turbolift", "target": "æ¶¡è½®ç”µæ¢¯", "type": "item", "note": ""},
            ])
        )
        st.session_state["corpora"] = {"base": normalize_glossary_df(fallback_df, corpus_name="base")}

    corpora = st.session_state["corpora"]

    st.markdown("**ä»ç›®å½•æ‰¹é‡å¯¼å…¥ CSV**ï¼ˆæ¯ä¸ª CSV è§†ä¸ºä¸€ä¸ªè¯­æ–™åº“ï¼Œæ–‡ä»¶åä¸ºè¯­æ–™åº“åï¼‰")
    colA, colB = st.columns([2, 1])
    with colA:
        corpora_dir = st.text_input("è¯­æ–™åº“ç›®å½•", value=str(Path.cwd() / "data/corpora"))
    with colB:
        if st.button("æ‰«æå¹¶å¯¼å…¥ç›®å½•"):
            p = Path(corpora_dir)
            if p.exists() and p.is_dir():
                count = 0
                for f in sorted(p.glob("*.csv")):
                    try:
                        df = pd.read_csv(f)
                        df = normalize_glossary_df(df, corpus_name=f.stem)
                        corpora[f.stem] = df
                        count += 1
                    except Exception as e:
                        st.warning(f"è·³è¿‡ {f.name}: {e}")
                st.success(f"å·²å¯¼å…¥ {count} ä¸ªè¯­æ–™åº“ã€‚")
            else:
                st.error("ç›®å½•ä¸å­˜åœ¨ã€‚")

    st.markdown("**ä¸Šä¼  CSV æ–°å¢è¯­æ–™åº“**ï¼ˆå­—æ®µ: source,target,type,noteï¼‰")
    up_files = st.file_uploader("é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ª CSV", type=["csv"], accept_multiple_files=True)
    if up_files:
        for uf in up_files:
            try:
                df = pd.read_csv(uf)
                df = normalize_glossary_df(df, corpus_name=Path(uf.name).stem)
                corpora[Path(uf.name).stem] = df
            except Exception as e:
                st.warning(f"è·³è¿‡ {uf.name}: {e}")
        st.success(f"å·²æ·»åŠ  {len(up_files)} ä¸ªè¯­æ–™åº“åˆ°ä¼šè¯ã€‚")

    with st.expander("â• æ–°å»ºç©ºç™½è¯­æ–™åº“", expanded=False):
        new_name = st.text_input("è¯­æ–™åº“åç§°", placeholder="ä¾‹å¦‚ ships/species/roles/items æˆ–ä»»æ„è‡ªå®šä¹‰")
        if st.button("åˆ›å»ºç©ºç™½è¯­æ–™åº“"):
            if not new_name.strip():
                st.warning("è¯·è¾“å…¥åç§°ã€‚")
            elif new_name in corpora:
                st.warning("è¯¥åç§°å·²å­˜åœ¨ã€‚")
            else:
                corpora[new_name] = normalize_glossary_df(
                    pd.DataFrame(columns=["source", "target", "type", "note"]),
                    corpus_name=new_name,
                )
                st.success(f"å·²åˆ›å»ºï¼š{new_name}")

    st.markdown("**ç¼–è¾‘è¯­æ–™åº“**")
    corpus_names = sorted(corpora.keys())
    sel = st.selectbox("é€‰æ‹©è¯­æ–™åº“", corpus_names, index=corpus_names.index("base") if "base" in corpus_names else 0)
    cur_df = corpora[sel].copy().drop(columns=["pattern"], errors="ignore")
    st.dataframe(cur_df, use_container_width=True, height=240)

    with st.expander("âœï¸ å°±åœ°ç¼–è¾‘å¹¶ä¿å­˜", expanded=False):
        edited = st.data_editor(
            cur_df,
            num_rows="dynamic",
            use_container_width=True,
            height=300,
            column_config={"source": "è‹±æ–‡", "target": "ä¸­æ–‡", "type": "ç±»åˆ«", "note": "å¤‡æ³¨", "corpus": "è¯­æ–™åº“"},
        )
        c1, c2, c3, c4 = st.columns(4)
        if c1.button("ä¿å­˜åˆ°å½“å‰è¯­æ–™åº“"):
            edited = normalize_glossary_df(edited, corpus_name=sel)
            corpora[sel] = edited
            st.success("å·²ä¿å­˜åˆ°ä¼šè¯ã€‚")
        if c2.button("ä¸‹è½½å½“å‰è¯­æ–™åº“ CSV"):
            st.download_button("ç‚¹æ­¤ä¸‹è½½", edited.to_csv(index=False).encode("utf-8"), file_name=f"{sel}.csv")
        if c3.button("åˆ é™¤è¯¥è¯­æ–™åº“"):
            if sel == "base":
                st.warning("åŸºç¡€è¯­æ–™åº“ base ä¸å»ºè®®åˆ é™¤ã€‚")
            else:
                del corpora[sel]
                st.experimental_rerun()
        with c4:
            st.caption("type å¸¸è§: ship/species/role/rank/item/org/place/techâ€¦")

    merged = merge_corpora(corpora)
    st.markdown("**åˆå¹¶æ€»è§ˆï¼ˆä»…å±•ç¤ºï¼Œä¸å«ç¼–è¯‘åˆ—ï¼‰**")
    st.dataframe(merged.drop(columns=["pattern"], errors="ignore"), use_container_width=True, height=260)

    if not chapter_text or not chapter_text.strip():
        st.info("Tab1 ç²˜è´´æ–‡æœ¬åï¼Œè¿™é‡Œå¯åšæœ¯è¯­å‘½ä¸­åˆ†æã€‚")
    else:
        gdf = build_patterns(merged.drop(columns=["pattern"], errors="ignore"))
        hits = find_hits(chapter_text, gdf)
        if hits:
            hit_df = pd.DataFrame([h.__dict__ for h in hits])
            st.dataframe(hit_df, use_container_width=True, height=240)
            st.download_button("ä¸‹è½½å‘½ä¸­æŠ¥å‘Š CSV", hit_df.to_csv(index=False).encode("utf-8"), "glossary_hits.csv")
        else:
            st.caption("æœªæ£€æµ‹åˆ°æœ¯è¯­å‘½ä¸­ã€‚")

# Tab3: è§„åˆ™ä¸ç³»ç»Ÿæç¤º + è°ƒç”¨
with T3:
    st.subheader("ç¿»è¯‘è§„åˆ™ (YAML)")
    rules_path = Path("data/rules_sample.yaml")
    default_yaml = rules_path.read_text(encoding="utf-8") if rules_path.exists() else None
    rules_text = st.text_area("ç¼–è¾‘/ç²˜è´´è§„åˆ™ YAMLï¼š", value=default_yaml, height=240)
    rules = parse_rules_yaml(rules_text)
    st.caption("è¿™äº›è§„åˆ™å°†æ³¨å…¥ç³»ç»Ÿæç¤ºï¼Œå¼ºåˆ¶äººåä¸ç¿»ã€èˆ°å/å†›è¡”/èŒä½/ç‰©ç§/ç‰©å“ç­‰ç»Ÿä¸€è¯‘åã€‚")

    if chapter_text and chapter_text.strip():
        corpora = st.session_state.get("corpora", {})
        merged = merge_corpora(corpora)
        gdf = build_patterns(merged.drop(columns=["pattern"], errors="ignore"))
        hits = find_hits(chapter_text, gdf)
        hit_terms = sorted({h.term for h in hits})
        glossary_subset = merged[merged["source"].isin(hit_terms)].copy() if hit_terms else merged.head(50).copy()
        names: List[str] = []  # äººåç”±æç¤ºè¯è‡ªæ£€æµ‹
        sys_prompt = build_system_prompt(rules, glossary_subset.drop(columns=["pattern"], errors="ignore"), names)

        st.markdown("**ç³»ç»Ÿæç¤ºï¼ˆç”¨äºè°ƒç”¨ç¿»è¯‘ï¼‰**")
        st.code(sys_prompt, language="json")

        rep = quick_qc(len(hits))
        with st.expander("å¿«é€Ÿè´¨é‡æ£€æŸ¥"):
            st.json({"glossary_hits": rep.glossary_hits, "violations": rep.violations})

        temp = 1.0
        st.caption("temperature å·²å›ºå®šä¸º 1ï¼ˆè¯¥æ¨¡å‹ä»…æ”¯æŒé»˜è®¤å€¼ï¼‰ã€‚")
        max_toks = st.number_input("max_output_tokens(å¯é€‰)", value=0, min_value=0, step=50, help="0 è¡¨ç¤ºä¸é™åˆ¶")
        resp_fmt = st.selectbox("response_format", ["text", "json"], index=0, help="è‹¥é€‰ jsonï¼Œå°†å‘é€ JSON schema(ç®€åŒ–ç¤ºä¾‹)")
        response_format = None
        if resp_fmt == "json":
            response_format = {"type": "json_object"}

        disabled = not (api_key or os.getenv("OPENAI_API_KEY"))
        if st.button("è¯•è¿è¡Œç¿»è¯‘", type="secondary", disabled=disabled):
            adapter = Translator(model, temperature=temp, max_output_tokens=(None if max_toks == 0 else max_toks), response_format=response_format, api_key=api_key)
            try:
                result = adapter.translate_once(system_prompt=sys_prompt, user_text=chapter_text[:4000])
                st.text_area("è¿”å›ç¤ºä¾‹", value=result.text, height=220)
                fname = st.text_input("å¯¼å‡ºæ–‡ä»¶å", value="translation.txt")
                st.download_button("ä¸‹è½½TXT", result.text.encode("utf-8"), file_name=fname, mime="text/plain")
                if result.meta.get("usage"):
                    st.caption(f"usage: {result.meta['usage']}")
            except Exception as e:
                st.error(f"è°ƒç”¨å¤±è´¥ï¼š{e}")
        elif disabled:
            st.info("è¯·åœ¨å·¦ä¾§è¾“å…¥ OpenAI API Key æˆ–è®¾ç½®ç¯å¢ƒå˜é‡åå†è¯•ã€‚")

        st.markdown("---")
        st.subheader("æ•´ç« ç¿»è¯‘ï¼ˆè‡ªåŠ¨åˆ†æ®µï¼‰")
        st.caption("æŒ‰ token é¢„ç®—è‡ªåŠ¨åˆ‡å—ï¼Œé€æ®µè°ƒç”¨å¹¶åˆå¹¶ä¸ºå…¨æ–‡ï¼›é€‚ç”¨äºé•¿ç« èŠ‚/æ•´ç« ã€‚")
        max_in_budget = st.number_input("æ¯æ®µæœ€å¤§è¾“å…¥ tokensï¼ˆä¸ºç³»ç»Ÿæç¤ºä¸è¾“å‡ºç•™ä½™é‡ï¼‰", value=6000, min_value=2000, max_value=24000, step=500)
        full_disabled = disabled or (not chapter_text.strip())
        if st.button("å¼€å§‹æ•´ç« ç¿»è¯‘", type="primary", disabled=full_disabled):
            adapter = Translator(model, temperature=1.0, max_output_tokens=(None if max_toks == 0 else max_toks), response_format=response_format, api_key=api_key)
            try:
                with st.spinner("æ•´ç« ç¿»è¯‘è¿›è¡Œä¸­â€¦"):
                    full_text = translate_full_text(adapter, sys_prompt, chapter_text, model, max_input_tokens=int(max_in_budget))
                st.success("æ•´ç« ç¿»è¯‘å®Œæˆ âœ…")
                st.text_area("å…¨æ–‡è¯‘æ–‡ï¼ˆé¢„è§ˆï¼‰", value=full_text, height=320)
                fname_full = st.text_input("å¯¼å‡ºæ–‡ä»¶åï¼ˆå…¨æ–‡ï¼‰", value="chapter_translation_full.txt", key="full_fn")
                st.download_button("ä¸‹è½½å…¨æ–‡ TXT", full_text.encode("utf-8"), file_name=fname_full, mime="text/plain")
            except Exception as e:
                st.error(f"æ•´ç« ç¿»è¯‘å¤±è´¥ï¼š{e}")

# Tab4: æ‰¹å¤„ç† JSONL + Batch æµç¨‹
with T4:
    st.subheader("æ‰¹å¤„ç†ï¼šä»æ–‡ä»¶å¤¹è¯»å–ç« èŠ‚ï¼Œç”Ÿæˆ Batch JSONL")
    colA, colB = st.columns([2, 1])
    with colA:
        folder = st.text_input("ç« èŠ‚æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆè¯»å– *.txtï¼‰", value=str(Path.cwd() / "chapters"))
    with colB:
        out_jsonl = st.text_input("è¾“å‡º JSONL è·¯å¾„", value=str(Path.cwd() / "batch" / "requests.jsonl"))

    st.caption("æµç¨‹ï¼šè¯»å–æ¯ä¸ªç« èŠ‚ â†’ åˆå¹¶è¯­æ–™åº“ â†’ ç”Ÿæˆç³»ç»Ÿæç¤ºï¼ˆæŒ‰å‘½ä¸­å­é›†ï¼›äººåç”±æç¤ºè¯è‡ªæ£€æµ‹ï¼‰â†’ ç»„è£…ä¸º /v1/chat/completions çš„ JSONL æ‰¹å¤„ç†æ–‡ä»¶ã€‚")

    if st.button("ç”Ÿæˆ JSONL"):
        chs = chapters_from_folder(folder)
        if not chs:
            st.error("æœªåœ¨è¯¥è·¯å¾„å‘ç° .txt ç« èŠ‚æ–‡ä»¶ã€‚")
        else:
            corpora = st.session_state.get("corpora", {})
            merged = merge_corpora(corpora)
            gdf = build_patterns(merged.drop(columns=["pattern"], errors="ignore"))
            rows: List[dict] = []
            for it in chs:
                text = it["text"]
                hits = find_hits(text, gdf)
                terms = sorted({h.term for h in hits})
                subset = merged[merged["source"].isin(terms)].copy() if terms else merged.head(50).copy()
                names: List[str] = []  # äººåç”±æç¤ºè¯è‡ªæ£€æµ‹
                system_prompt = build_system_prompt(rules, subset.drop(columns=["pattern"], errors="ignore"), names)
                rows.append({"id": it["id"], "system_prompt": system_prompt, "user_text": text})
            adapter = Translator(model, api_key=api_key)
            jsonl_rows = adapter.prepare_batch_items(rows)
            build_batch_jsonl(jsonl_rows, out_jsonl)
            st.success(f"å·²ç”Ÿæˆï¼š{out_jsonl}")
            p = Path(out_jsonl)
            if p.exists():
                st.code(p.read_text(encoding="utf-8")[:1200] + "\n...", language="json")

    st.markdown("---")
    st.subheader("Batch API è¿è¡Œï¼šä¸Šä¼  â†’ åˆ›å»ºä»»åŠ¡ â†’ æŸ¥è¯¢ â†’ ä¸‹è½½ç»“æœ")

    if "batch_state" not in st.session_state:
        st.session_state["batch_state"] = {"input_file_id": None, "batch_id": None, "output_file_id": None}
    bstate = st.session_state["batch_state"]

    disabled_batch = not (api_key or os.getenv("OPENAI_API_KEY"))
    col1, col2 = st.columns(2)
    with col1:
        if st.button("â‘  ä¸Šä¼  JSONL å¹¶åˆ›å»º Batch ä»»åŠ¡", type="primary", disabled=disabled_batch):
            try:
                client = _make_client_from_gui(api_key)
                up = client.files.create(file=open(out_jsonl, "rb"), purpose="batch")
                bstate["input_file_id"] = up.id
                job = client.batches.create(input_file_id=up.id, endpoint="/v1/chat/completions", completion_window="24h")
                bstate["batch_id"] = job.id
                st.success(f"å·²åˆ›å»º Batchï¼š{job.id}")
                st.json({"batch_id": job.id, "status": job.status, "input_file_id": up.id})
            except Exception as e:
                st.error(f"åˆ›å»ºå¤±è´¥ï¼š{e}")
    with col2:
        if st.button("â‘¡ æŸ¥è¯¢/åˆ·æ–°çŠ¶æ€", disabled=disabled_batch or not bstate.get("batch_id")):
            try:
                client = _make_client_from_gui(api_key)
                job = client.batches.retrieve(bstate["batch_id"])
                st.info(f"çŠ¶æ€ï¼š{job.status}")
                if getattr(job, "output_file_id", None):
                    bstate["output_file_id"] = job.output_file_id
                st.json({
                    "status": job.status,
                    "request_counts": getattr(job, "request_counts", None) and job.request_counts.model_dump() or {},
                    "created_at": getattr(job, "created_at", None),
                    "output_file_id": getattr(job, "output_file_id", None),
                })
            except Exception as e:
                st.error(f"æŸ¥è¯¢å¤±è´¥ï¼š{e}")

    st.caption("å½“çŠ¶æ€ä¸º completed æ—¶ï¼Œå¯ä¸‹è½½ç»“æœã€‚ç»“æœå¯èƒ½ä¹±åºï¼Œè¯·ä½¿ç”¨ custom_id å›ç»‘ç« èŠ‚ã€‚")

    col3, col4 = st.columns(2)
    with col3:
        default_results_path = str(Path.cwd() / "batch" / "results.jsonl")
        res_path = st.text_input("ä¿å­˜ç»“æœ JSONL åˆ°ï¼š", value=default_results_path)
        if st.button("â‘¢ ä¸‹è½½ç»“æœ JSONL", disabled=disabled_batch or not bstate.get("output_file_id")):
            try:
                client = _make_client_from_gui(api_key)
                _save_results_jsonl_bytes(client, bstate["output_file_id"], Path(res_path))
                st.success(f"å·²ä¿å­˜ï¼š{res_path}")
                st.code(Path(res_path).read_text(encoding="utf-8")[:800] + "\n...", language="json")
            except Exception as e:
                st.error(f"ä¸‹è½½å¤±è´¥ï¼š{e}")
    with col4:
        out_dir = st.text_input("æŒ‰ custom_id è¾“å‡ºåˆ°ç›®å½•ï¼š", value=str(Path.cwd() / "batch_outputs"))
        if st.button("â‘£ è§£æå¹¶å¯¼å‡º TXT", disabled=not Path(res_path).exists()):
            try:
                results = _parse_results_jsonl(Path(res_path))
                paths = _write_txt_outputs(results, Path(out_dir))
                zip_bytes = _zip_paths(paths)
                st.success(f"å·²å†™å…¥ {len(paths)} ä¸ªç« èŠ‚åˆ° {out_dir}")
                st.download_button("ä¸‹è½½æ‰€æœ‰ç« èŠ‚ï¼ˆZIPï¼‰", data=zip_bytes, file_name="batch_outputs.zip")
            except Exception as e:
                st.error(f"è§£æå¤±è´¥ï¼š{e}")

# Tab5: æˆå“è´¨æ£€/ä¿®å¤
with T5:
    st.subheader("å¯¹å·²ç¿»è¯‘ TXT åšä¸€è‡´æ€§è´¨æ£€ä¸å¿«é€Ÿä¿®å¤")
    st.caption("æ£€æŸ¥ç‚¹ï¼šâ‘  æœ¯è¯­è¦†ç›–ï¼ˆè‹±æ–‡åŸæ–‡å‘½ä¸­ â†’ è¯‘æ–‡æ˜¯å¦åŒ…å« targetï¼‰ï¼›â‘¡ å†›è¡”é¡ºåºï¼ˆåº”ä¸ºâ€œåå­— åœ¨å‰ï¼Œå†›è¡” åœ¨åâ€ï¼‰ï¼›â‘¢ æ®µè½ç©ºè¡Œï¼ˆæ®µè½é—´è‡³å°‘ä¸€ä¸ªç©ºè¡Œï¼‰ã€‚")

    # ä¸Šä¼ ï¼šä½¿ç”¨ getvalue()ï¼Œé¿å…è¢« .read() æ¶ˆè€—
    colL, colR = st.columns(2)
    with colL:
        en_up = st.file_uploader("ä¸Šä¼ è‹±æ–‡åŸæ–‡ TXTï¼ˆç”¨äºæœ¯è¯­å‘½ä¸­ï¼‰", type=["txt"], key="qc_en")
    with colR:
        zh_up = st.file_uploader("ä¸Šä¼ ä¸­æ–‡è¯‘æ–‡ TXTï¼ˆå¾…è´¨æ£€/ä¿®å¤ï¼‰", type=["txt"], key="qc_zh")

    # è¯­æ–™åº“åˆå¹¶ & å†›è¡”è¯è¡¨
    corpora = st.session_state.get("corpora", {})
    merged = _merge_corpora_for_check(corpora)
    ranks = _collect_rank_words(merged)

    # æŠŠä¸Šä¼ å†…å®¹ç¼“å­˜åˆ° session_state
    if en_up:
        st.session_state["qc_en_text"] = en_up.getvalue().decode("utf-8", errors="ignore")
    if zh_up:
        st.session_state["qc_zh_text"] = zh_up.getvalue().decode("utf-8", errors="ignore")

    en_text = st.session_state.get("qc_en_text", "")
    zh_text = st.session_state.get("qc_zh_text", "")

    # å¼€å§‹è´¨æ£€ï¼šå†™å…¥ session_state æ–¹ä¾¿åç»­æ“ä½œ
    if st.button("å¼€å§‹è´¨æ£€", disabled=not (en_text and zh_text)):
        cov_df = _glossary_coverage(en_text, zh_text, merged)
        issues = _find_rank_order_issues(zh_text, ranks)
        fixed_para = _normalize_paragraphs(zh_text)

        st.session_state["qc_cov_df"] = cov_df
        st.session_state["qc_issues"] = issues
        st.session_state["qc_fixed_para"] = fixed_para
        st.session_state["qc_ranks"] = ranks

        st.success("è´¨æ£€å®Œæˆï¼ˆç»“æœå·²ç¼“å­˜ï¼Œå¯åœ¨ä¸‹æ–¹æŸ¥çœ‹/ä¿®å¤ï¼‰ã€‚")

    # ä» session_state å–ç»“æœ
    cov_df = st.session_state.get("qc_cov_df", None)
    issues = st.session_state.get("qc_issues", None)
    fixed_para = st.session_state.get("qc_fixed_para", None)
    ranks_cached = st.session_state.get("qc_ranks", ranks)

    # â€”â€” æœ¯è¯­è¦†ç›–æŠ¥å‘Š â€”â€” 
    st.markdown("### æœ¯è¯­è¦†ç›–æŠ¥å‘Šï¼ˆå«è‡ªåŠ¨ä¿®å¤ä¿¡æ¯ï¼‰")
    if cov_df is None:
        st.info("è¯·å…ˆç‚¹å‡»â€œå¼€å§‹è´¨æ£€â€ã€‚")
    else:
        if cov_df.empty:
            st.info("æœªè¯†åˆ«åˆ°ä»»ä½•è¯­æ–™åº“å‘½ä¸­ï¼Œæˆ–è¯­æ–™åº“ä¸ºç©ºã€‚")
        else:
            # æ ‡å‡ºâ€œéœ€è¦ä¿®å¤â€çš„æ¡ç›®ï¼šè‹±æ–‡å‘½ä¸­ + è¯‘æ–‡æ—  target + è¯‘æ–‡ä»æ®‹ç•™ source
            need_fix_mask = (
                (cov_df["hit_in_english"] == True) &
                (cov_df["found_target_in_zh"] == False) &
                (cov_df["found_source_in_zh"] == True)
            )
            cov_df_show = cov_df.copy()
            cov_df_show["need_auto_fix"] = need_fix_mask

            st.dataframe(cov_df_show, use_container_width=True, height=260)
            st.download_button(
                "ä¸‹è½½è¦†ç›–æŠ¥å‘Š CSV",
                cov_df_show.to_csv(index=False).encode("utf-8"),
                file_name="coverage_report.csv",
            )

            n_need_fix = int(need_fix_mask.sum())
            if n_need_fix > 0:
                st.warning(f"æœ‰ {n_need_fix} æ¡æœ¯è¯­åœ¨è‹±æ–‡ä¸­å‡ºç°ï¼Œä½†è¯‘æ–‡æœ¬èº«ä»ç•™è‹±æ–‡ä¸”æœªä½¿ç”¨è¯­æ–™åº“ targetï¼Œå°†åœ¨ä¸€é”®ä¿®å¤ä¸­è‡ªåŠ¨æ›¿æ¢ã€‚")
            else:
                st.success("æœªå‘ç°éœ€è¦è‡ªåŠ¨ä¿®å¤çš„æœ¯è¯­ï¼ˆæˆ–æ— æ³•åˆ¤æ–­ï¼‰ã€‚")

    # â€”â€” å†›è¡”é¡ºåºæ£€æŸ¥ â€”â€” 
    st.markdown("### å†›è¡”é¡ºåºæ£€æŸ¥ï¼ˆåº”ä¸ºâ€œåå­— åœ¨å‰ï¼Œå†›è¡” åœ¨åâ€ï¼‰")
    if issues is None:
        st.info("è¯·å…ˆç‚¹å‡»â€œå¼€å§‹è´¨æ£€â€ã€‚")
    else:
        if not issues:
            st.success("æœªå‘ç°å†›è¡”åœ¨å‰ã€åå­—åœ¨åçš„è¿ä¾‹ã€‚")
        else:
            preview_rows = []
            for rk, nm, a, b in issues[:200]:
                snippet = zh_text[max(0, a - 20):min(len(zh_text), b + 20)].replace("\n", " ")
                preview_rows.append({"rank": rk, "name": nm, "context": snippet})
            st.dataframe(pd.DataFrame(preview_rows), use_container_width=True, height=220)
            st.caption(f"å…±å‘ç° {len(issues)} å¤„ã€‚")

    # â€”â€” æ®µè½ç©ºè¡Œ â€”â€” 
    st.markdown("### æ®µè½ç©ºè¡Œ")
    if fixed_para is None:
        st.info("è¯·å…ˆç‚¹å‡»â€œå¼€å§‹è´¨æ£€â€ã€‚")
    else:
        if fixed_para != zh_text:
            st.info("æ£€æµ‹åˆ°æ®µè½ç©ºè¡Œé—®é¢˜ï¼Œå·²ç”Ÿæˆä¿®å¤ç‰ˆæœ¬ï¼ˆå°†åœ¨ä¸€é”®ä¿®å¤ä¸­ä½¿ç”¨ï¼‰ã€‚")
        else:
            st.success("æ®µè½ç©ºè¡Œçœ‹èµ·æ¥æ­£å¸¸ã€‚")

    st.markdown("---")
    st.subheader("ä¸€é”®ä¿®å¤ï¼ˆè¯­æ–™åº“æœ¯è¯­ + å†›è¡”é¡ºåº + æ®µè½ç©ºè¡Œï¼‰")

    st.checkbox("è‡ªåŠ¨æŠŠ â€˜å†›è¡” åå­—â€™ äº’æ¢ä¸º â€˜åå­— å†›è¡”â€™ï¼ˆä»…è‹±æ–‡åå­—åœºæ™¯ï¼‰", key="qc_do_swap", value=False)
    fix_filename = st.text_input("ä¿å­˜æ–‡ä»¶å", value="translated_fixed.txt", key="qc_fix_filename")

    apply_disabled = fixed_para is None
    if st.button("åº”ç”¨ä¿®å¤å¹¶ä¸‹è½½ TXT", disabled=apply_disabled):
        cov_df = st.session_state.get("qc_cov_df", None)
        fixed_para = st.session_state.get("qc_fixed_para", zh_text)
        issues = st.session_state.get("qc_issues", [])
        ranks_cached = st.session_state.get("qc_ranks", ranks)

        # 1) å…ˆç”¨è¯­æ–™åº“åšæœ¯è¯­ä¿®å¤ï¼ˆè‹±æ–‡ source â†’ ä¸­æ–‡ targetï¼‰
        out_txt = _apply_glossary_repairs(fixed_para or zh_text, cov_df)

        # 2) å¯é€‰ï¼šå†›è¡”é¡ºåºä¿®å¤
        if st.session_state.get("qc_do_swap", False) and issues:
            out_txt = _auto_fix_rank_order(out_txt, ranks_cached)

        # 3) ä¸‹è½½
        st.download_button(
            "ä¸‹è½½ä¿®å¤å TXT",
            data=out_txt.encode("utf-8"),
            file_name=st.session_state.get("qc_fix_filename", "translated_fixed.txt"),
            mime="text/plain",
        )